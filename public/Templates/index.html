<!DOCTYPE html>
<html>

<head>
  <title>Talking Head - MP3 example</title>
  <style>
    html,
    body {
      background-color: #202020;
      margin: 0;
      padding: 0;
      font-family: Arial, sans-serif;
      width: 100%;
      height: 100%;
      overflow: hidden;
      /* Prevent scrollbars triggering resize loops */
    }

    #background {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-image: url('/backgrounds/classroom_bg.png');
      background-size: cover;
      background-position: center;
      z-index: 0;
    }

    #avatar {
      display: block;
      position: absolute;
      top: 0;
      left: 0;
      width: 30%;
      height: 100%;
      z-index: 1;
    }

    /* Whiteboard Board Image Container */
    #whiteboardBoard {
      position: absolute;
      top: 5%;
      right: 2%;
      width: 65%;
      height: 85%;
      z-index: 2;
      opacity: 0;
      transition: opacity 0.5s ease;
      pointer-events: none;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    #whiteboardBoard.visible {
      opacity: 1;
    }

    /* Board backdrop blur REMOVED as per request to avoid double-board look */
    /* #whiteboardBoard::before { ... } */

    /* Actual board image */
    #whiteboardBoard img {
      width: 100%;
      height: 100%;
      object-fit: contain;
      /* Enhanced shadow for depth */
      filter: drop-shadow(0 8px 30px rgba(0, 0, 0, 0.5));
    }

    /* Text visually inside the board */
    /* Text visually inside the board */
    #whiteboardTextContent {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      width: 80%;
      max-height: 80%;

      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;

      color: #1a1a1a;
      font-size: 16px;
      line-height: 1.5;
      text-align: center;
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      font-weight: 600;

      overflow-y: auto;
      padding: 10px;
    }

    /* Scrollbar for board text */
    #whiteboardTextContent::-webkit-scrollbar {
      width: 4px;
    }

    #whiteboardTextContent::-webkit-scrollbar-thumb {
      background: rgba(0, 0, 0, 0.3);
      border-radius: 2px;
    }

    #controls {
      display: flex;
      flex-direction: column;
      gap: 10px;
      position: absolute;
      top: 50px;
      left: calc(60% + 50px);
      right: 50px;
      bottom: 50px;
      z-index: 10;
    }

    #json {
      width: 100%;
      height: 600px;
      background-color: rgba(255, 255, 255, 0.2);
      color: white;
      font-size: 14px;
      border: none;
      padding: 5px 10px;
      resize: none;
      overflow: hidden;
    }

    #loading {
      position: absolute;
      top: 50px;
      left: 50px;
      font-size: 20px;
      color: white;
      z-index: 10;
    }

    button {
      font-size: 20px;
      padding: 5px 10px;
      cursor: pointer;
    }

    #unlockAudio {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;

      opacity: 0;
      /* Invisible */
      border: none;
      outline: none;
      background: transparent;

      z-index: 999999;
      /* On top of everything */
      cursor: pointer;
    }

    /* Narration Text Overlay */
    #narrationOverlay {
      position: fixed;
      bottom: 120px;
      left: 50%;
      transform: translateX(-50%);
      max-width: 90%;
      width: auto;
      padding: 15px 25px;
      background: rgba(253, 253, 253, 0.9);
      backdrop-filter: blur(10px);
      border-radius: 12px;
      border: 1px solid rgba(255, 255, 255, 0.15);
      z-index: 5;
      opacity: 0;
      transition: opacity 0.3s ease;
      pointer-events: none;
    }

    #narrationOverlay.visible {
      opacity: 1;
    }

    #narrationText {
      color: white;
      font-size: 16px;
      line-height: 1.5;
      text-align: center;
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-height: 150px;
      overflow-y: auto;
    }

    /* Hide scrollbar but keep functionality */
    #narrationText::-webkit-scrollbar {
      width: 4px;
    }

    #narrationText::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 2px;
    }
  </style>

  <script type="importmap">
  {
    "imports": {
      "three": "https://cdn.jsdelivr.net/npm/three@0.180.0/build/three.module.js/+esm",
      "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.180.0/examples/jsm/",
      "talkinghead": "https://cdn.jsdelivr.net/gh/met4citizen/TalkingHead@1.5/modules/talkinghead.mjs"
    }
  }
  </script>

  <script type="module">
    // --- Audio Capture Logic (Monkey Patch) - MUST BE FIRST ---
    let audioDest;
    const originalConnect = AudioNode.prototype.connect;
    AudioNode.prototype.connect = function (destination, outputIndex, inputIndex) {
      if (audioDest && destination === destination.context.destination) {
        try {
          originalConnect.call(this, audioDest);
        } catch (e) { }
      }
      return originalConnect.apply(this, arguments);
    };

    import { TalkingHead } from "talkinghead";
    import * as THREE from "three";

    const nodeAvatar = document.getElementById('avatar');
    const nodeLoading = document.getElementById('loading');
    const narrationOverlay = document.getElementById('narrationOverlay');
    const narrationText = document.getElementById('narrationText');
    const whiteboardText = document.getElementById('whiteboardBoard'); // Updated to use Board Container
    const whiteboardTextContent = document.getElementById('whiteboardTextContent');

    // --- State Management ---
    const STATE = {
      IDLE: 'IDLE',
      PRELOADING: 'PRELOADING',
      PLAYING: 'PLAYING',
      PAUSED: 'PAUSED',
      WAITING_FOR_QUESTION: 'WAITING_FOR_QUESTION',
      CHAT_MODE: 'CHAT_MODE'
    };
    let currentState = STATE.IDLE;

    let lectureData = [];
    let preloadedSlides = [];
    let currentSlideIndex = 0;

    let mediaRecorder;
    let recordedChunks = [];

    // Initialize TalkingHead with dummy TTS endpoint (we'll use browser SpeechSynthesis instead)
    const head = new TalkingHead(nodeAvatar, {
      ttsEndpoint: "https://texttospeech.googleapis.com/v1beta1/text:synthesize",
      ttsApikey: "dummy", // Won't be used, but library requires it
      lipsyncModules: ["en", "fi"],
      cameraView: "full"
    });

    audioDest = head.audioCtx.createMediaStreamDestination();

    // --- Voice Recognition Setup ---
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;
    if (SpeechRecognition) {
      recognition = new SpeechRecognition();
      recognition.lang = 'en-IN';
      recognition.continuous = false;
      recognition.interimResults = false;

      recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript.toLowerCase();
        console.log("Voice Input:", transcript);
        const yesKeywords = ["yes", "ha", "haan", "haa", "yeah", "yep", "sure"];
        if (yesKeywords.some(kw => transcript.includes(kw))) {
          handleYesResponse();
        }
      };

      recognition.onend = () => {
        if (currentState === STATE.WAITING_FOR_QUESTION) {
          // If we finished listening and no "yes" was caught, the 5s timer will handle the fallback
        }
      };
    }

    // --- Preloading Logic ---

    async function loadLectureData() {
      if (currentState !== STATE.IDLE) return;
      currentState = STATE.PRELOADING;

      try {
        nodeLoading.textContent = "Analyzing Lecture...";
        const lectureId = new URLSearchParams(window.location.search).get('lectureId');
        if (!lectureId) throw new Error("No Lecture ID provided");

        const res = await fetch(`https://api.edinai.inaiverse.com/lectures/${lectureId}/play`);
        if (!res.ok) throw new Error("Failed to fetch lecture");
        const data = await res.json();
        const detailUrl = "https://api.edinai.inaiverse.com" + data.lecture_url;

        const detailRes = await fetch(detailUrl);
        const detailData = await detailRes.json();

        lectureData = (detailData.slides || []).map(slide => ({
          audio_url: "https://api.edinai.inaiverse.com" + slide.audio_url,
          narration: slide.narration || ""
        }));

        await preloadSlides();

        // Only set to IDLE if we haven't started playing yet
        if (currentState === STATE.PRELOADING) {
          currentState = STATE.IDLE;
          nodeLoading.textContent = "Ready to Begin";
          // Send READY again just in case
          window.parent.postMessage({ type: 'EVT_READY' }, '*');
        }

      } catch (e) {
        console.error(e);
        nodeLoading.textContent = "Preload Error: " + e.message;
      }
    }

    async function preloadSlides() {
      for (let i = 0; i < lectureData.length; i++) {
        // If first slide is already done, show generic loading
        if (i > 0) {
          nodeLoading.textContent = `Preloading Slide ${i + 1}/${lectureData.length}...`;
        } else {
          nodeLoading.textContent = `Loading First Slide...`;
        }

        try {
          const slide = lectureData[i];
          const prepared = await prepareSlideAudio(slide.audio_url, slide.narration);
          if (prepared) preloadedSlides.push(prepared);

          // Early Ready Signal: After 1st slide
          if (i === 0) {
            console.log('‚úÖ First slide loaded - Enabling Start');
            window.parent.postMessage({ type: 'EVT_READY' }, '*');
            nodeLoading.textContent = "Ready to Begin (Background loading...)";

            // Allow state transition to IDLE or Playable so buttons work
            // But we keep 'currentState' as PRELOADING locally until loadLectureData finishes?
            // No, React side needs EVT_READY. React side doesn't care about iframe internal state for the button enable
            // But the 'CMD_START' handler in iframe needs to work.

            // We need to briefly set state to IDLE so CMD_START works?
            // Actually CMD_START logic:
            // if (mediaRecorder.state === 'inactive') { ... startLectureLoop() }
            // It doesn't strictly check 'currentState' to allow start, it checks mediaRecorder.

            // But 'loadLectureData' sets 'currentState = STATE.PRELOADING' at start.
            // Let's allow it.
          }

        } catch (e) {
          console.error("Failed to preload slide " + i, e);
        }
      }
    }

    async function prepareSlideAudio(url, narration) {
      const resp = await fetch(url);
      const blob = await resp.blob();
      const file = new File([blob], "audio.mp3", { type: blob.type });

      const ab = await file.arrayBuffer();
      const audioBuffer = await head.audioCtx.decodeAudioData(ab);

      let words = [], wtimes = [], wdurations = [];
      try {
        const form = new FormData();
        form.append("file", file);
        form.append("model", "whisper-large-v3-turbo");
        form.append("language", "en");
        form.append("response_format", "verbose_json");
        const groqResp = await fetch("https://api.groq.com/openai/v1/audio/transcriptions", {
          method: "POST",
          body: form,
          headers: { "Authorization": `Bearer ${import.meta.env.VITE_GROQ_API_KEY}` }
        });

        if (groqResp.ok) {
          const json = await groqResp.ok ? await groqResp.json() : null;
          if (json && json.segments) {
            json.segments.forEach(s => {
              words.push(s.text.trim());
              wtimes.push(Math.round(s.start * 1000));
              wdurations.push(Math.round((s.end - s.start) * 1000));
            });
          }
        }
      } catch (e) { }

      return { audio: audioBuffer, words, wtimes, wdurations, text: narration };
    }

    // --- Core State Logic ---

    async function setMachineState(newState) {
      console.log(`üîÑ State Transition: ${currentState} ‚Üí ${newState}`);
      const oldState = currentState;
      currentState = newState;

      // Notify parent React component
      window.parent.postMessage({
        type: 'EVT_SYNC_STATE',
        state: newState,
        oldState: oldState,
        timestamp: Date.now(),
        slideIndex: currentSlideIndex
      }, '*');

      // Handle state-specific actions
      if (newState === STATE.PAUSED || newState === STATE.WAITING_FOR_QUESTION || newState === STATE.CHAT_MODE) {
        // Pause everything
        console.log(`‚è∏Ô∏è  Pausing: Recording=${mediaRecorder?.state}, Audio=${head.audioCtx.state}`);

        if (mediaRecorder && mediaRecorder.state === 'recording') {
          try {
            mediaRecorder.pause();
            console.log('‚úÖ Recording paused');
          } catch (e) {
            console.error('‚ùå Failed to pause recording:', e);
          }
        }

        if (head.audioCtx.state === 'running') {
          try {
            await head.audioCtx.suspend();
            console.log('‚úÖ Audio context suspended');
          } catch (e) {
            console.error('‚ùå Failed to suspend audio:', e);
          }
        }

        head.stop();
        console.log('‚úÖ Avatar stopped');
      }
      else if (newState === STATE.PLAYING) {
        // Resume everything
        console.log(`‚ñ∂Ô∏è  Resuming: Recording=${mediaRecorder?.state}, Audio=${head.audioCtx.state}`);

        if (head.audioCtx.state === 'suspended') {
          try {
            await head.audioCtx.resume();
            console.log('‚úÖ Audio context resumed');
          } catch (e) {
            console.error('‚ùå Failed to resume audio:', e);
          }
        }

        if (mediaRecorder && mediaRecorder.state === 'paused') {
          try {
            mediaRecorder.resume();
            console.log('‚úÖ Recording resumed');
          } catch (e) {
            console.error('‚ùå Failed to resume recording:', e);
          }
        }

        head.start();
        console.log('‚úÖ Avatar started');
      }
    }

    async function startLectureLoop() {
      console.log('üé¨ Starting lecture loop...');
      console.log(`üìä Preloaded slides: ${preloadedSlides.length}`);
      console.log(`üìä Current slide index: ${currentSlideIndex}`);

      if (preloadedSlides.length === 0) {
        console.error('‚ùå No slides preloaded!');
        return;
      }

      if (head.audioCtx.state === 'suspended') {
        console.log('üîä Resuming audio context...');
        await head.audioCtx.resume();
      }

      await setMachineState(STATE.PLAYING);

      for (; currentSlideIndex < preloadedSlides.length; currentSlideIndex++) {
        const slideAudio = preloadedSlides[currentSlideIndex];
        console.log(`\nüìç Processing slide ${currentSlideIndex + 1}/${preloadedSlides.length}`);

        // Check for external Pause OR Chat Mode
        while (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) {
          await new Promise(r => setTimeout(r, 200));
        }

        if (slideAudio) {
          // Show narration text ON WHITEBOARD
          if (slideAudio.text) {
            whiteboardTextContent.textContent = slideAudio.text;

            // Adaptive Font Sizing for Board Containment
            const len = slideAudio.text.length;
            // "By default perfect small rakho" -> Start conservative
            if (len > 350) whiteboardTextContent.style.fontSize = "12px";
            else if (len > 250) whiteboardTextContent.style.fontSize = "14px";
            else if (len > 150) whiteboardTextContent.style.fontSize = "15px";
            else whiteboardTextContent.style.fontSize = "16px";

            whiteboardText.classList.add('visible');
            console.log(`üìù Displaying on whiteboard (len:${len}): ${slideAudio.text.substring(0, 50)}...`);
          }

          console.log('üéµ Starting audio playback...');
          head.speakAudio(slideAudio);
          await waitForAudioEndOrPause();
          console.log('‚úÖ Audio playback finished');

          // Hide whiteboard text when slide ends
          whiteboardText.classList.remove('visible');

          // If we paused during the slide, wait here
          if (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) {
            console.log('‚è∏Ô∏è  Paused during slide, will replay');
            currentSlideIndex--; // Replay / Resume this slide
            continue;
          }

          // Slide Ended -> Trigger Prompt
          await handleSlideBoundary();

          // CRITICAL: After handleSlideBoundary, check if we entered CHAT_MODE
          // If yes, we need to HOLD here and wait for explicit resume
          while (currentState === STATE.CHAT_MODE) {
            await new Promise(r => setTimeout(r, 200));
          }

          // If we're still paused after chat, replay this slide
          if (currentState === STATE.PAUSED) {
            currentSlideIndex--;
            continue;
          }
        }
      }

      console.log('üèÅ Lecture loop completed');
      finishLecture();
    }

    async function waitForAudioEndOrPause() {
      return new Promise(resolve => {
        const check = () => {
          if (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) { resolve(); return; }
          if (!head.isAudioPlaying) resolve();
          else setTimeout(check, 100);
        };
        check();
      });
    }

    async function handleSlideBoundary() {
      // Automatically pause all
      await setMachineState(STATE.WAITING_FOR_QUESTION);

      // --- 1. Wait exactly 5 seconds after slide ends ---
      console.log("Slide ended. Waiting 5s before prompt...");
      let slideWait = 0;
      while (slideWait < 5000 && currentState === STATE.WAITING_FOR_QUESTION) {
        await new Promise(r => setTimeout(r, 100));
        slideWait += 100;
      }

      // CRITICAL: Check if state changed (e.g., user paused or said yes)
      if (currentState !== STATE.WAITING_FOR_QUESTION) {
        console.log("State changed during initial wait. Aborting question prompt.");
        return;
      }

      // --- 2. System asks verbally using browser TTS ---
      const questionText = "Do you have any questions?";
      const utterance = new SpeechSynthesisUtterance(questionText);
      utterance.rate = 1;
      utterance.pitch = 1;
      utterance.volume = 1;
      utterance.lang = "en-US";

      window.speechSynthesis.speak(utterance);
      console.log(`üó£Ô∏è Speaking: "${questionText}"`);

      // Wait for speech to finish
      await new Promise(resolve => {
        utterance.onend = () => {
          console.log('‚úÖ Question prompt finished');
          resolve();
        };
        utterance.onerror = (e) => {
          console.error('‚ùå Speech error:', e);
          resolve();
        };

        // Fallback timeout
        setTimeout(resolve, 5000);
      });

      // Check again after speaking
      if (currentState !== STATE.WAITING_FOR_QUESTION) {
        console.log("State changed during question prompt. Aborting.");
        return;
      }

      // --- 3. Open microphone ---
      if (recognition && currentState === STATE.WAITING_FOR_QUESTION) {
        try { recognition.start(); } catch (e) { console.warn("Recognition start failed:", e); }
      }

      // --- 4. Wait for user response (5-7 seconds) ---
      let responseWait = 0;
      const maxResponseWait = 6000;
      while (responseWait < maxResponseWait && currentState === STATE.WAITING_FOR_QUESTION) {
        await new Promise(r => setTimeout(r, 100));
        responseWait += 100;
      }

      // Cleanup recognition
      if (recognition) {
        try { recognition.stop(); } catch (e) { }
      }

      // --- 5. Auto-move to next slide ONLY if still in WAITING_FOR_QUESTION state ---
      if (currentState === STATE.WAITING_FOR_QUESTION) {
        console.log("No response detected. Moving to next slide.");
        await setMachineState(STATE.PLAYING);
      } else {
        console.log("User responded or state changed. Not auto-progressing.");
      }
    }

    async function handleYesResponse() {
      if (currentState !== STATE.WAITING_FOR_QUESTION) return;
      await setMachineState(STATE.CHAT_MODE);
      window.parent.postMessage({ type: 'EVT_VOICE_TRIGGER', response: 'YES' }, '*');
    }

    function finishLecture() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
      window.parent.postMessage({ type: "EVT_LECTURE_COMPLETED" }, "*");
    }

    // --- Messaging ---

    window.addEventListener('message', async e => {
      const { type, text } = e.data;

      switch (type) {
        case 'CMD_START':
          if (!mediaRecorder) {
            setupRecorder();
            if (!mediaRecorder) {
              console.error('‚ùå Failed to setup recorder');
              return;
            }
          }

          if (mediaRecorder.state === 'inactive') {
            try {
              // Start with timeslice to ensure regular chunks
              mediaRecorder.start(1000); // Request data every 1 second
              console.log('üî¥ Recording started with 1s timeslice');
              startLectureLoop();
            } catch (e) {
              console.error('‚ùå Failed to start recording:', e);
            }
          }
          break;
        case 'CMD_PAUSE':
          await setMachineState(STATE.PAUSED);
          break;
        case 'CMD_ENTER_CHAT':
          // User manually opened chat - enter CHAT_MODE
          console.log('üí¨ Entering CHAT_MODE (manual)');
          await setMachineState(STATE.CHAT_MODE);
          break;
        case 'CMD_RESUME':
          // Stop any active voice recognition
          if (recognition) {
            try { recognition.stop(); } catch (e) { }
          }
          // Exit chat mode and resume playing
          if (currentState === STATE.CHAT_MODE || currentState === STATE.WAITING_FOR_QUESTION || currentState === STATE.PAUSED) {
            console.log("Resuming from", currentState);
            await setMachineState(STATE.PLAYING);
          }
          break;
        case 'CMD_CHAT_REPLY':
          // Speak chatbot response using browser TTS
          if (text) {
            // Stop any ongoing speech
            window.speechSynthesis.cancel();

            const chatUtterance = new SpeechSynthesisUtterance(text);
            chatUtterance.rate = 1;
            chatUtterance.pitch = 1;
            chatUtterance.volume = 1;
            chatUtterance.lang = "en-US";

            window.speechSynthesis.speak(chatUtterance);
            console.log(`ü§ñ Chatbot speaking: "${text.substring(0, 50)}..."`);
          }
          break;
        case 'CMD_STOP':
          finishLecture();
          break;
      }
    });

    // --- Setup ---

    function setupRecorder() {
      console.log('üé¨ Setting up recorder...');

      const canvas = document.querySelector('#avatar canvas') || document.querySelector('canvas');
      if (!canvas) {
        console.error('‚ùå Canvas not found!');
        return;
      }

      const videoStream = canvas.captureStream(30);
      const combined = new MediaStream([
        ...videoStream.getVideoTracks(),
        ...audioDest.stream.getAudioTracks()
      ]);

      // Try different codecs in order of preference
      const codecOptions = [
        { mimeType: 'video/mp4;codecs="avc1.42E01E,mp4a.40.2"', ext: 'mp4', name: 'MP4 (H.264)' },
        { mimeType: 'video/webm;codecs=vp9,opus', ext: 'webm', name: 'WebM (VP9)' },
        { mimeType: 'video/webm;codecs=vp8,opus', ext: 'webm', name: 'WebM (VP8)' },
        { mimeType: 'video/webm', ext: 'webm', name: 'WebM (default)' }
      ];

      let selectedCodec = null;
      for (const codec of codecOptions) {
        if (MediaRecorder.isTypeSupported(codec.mimeType)) {
          selectedCodec = codec;
          console.log(`‚úÖ Using codec: ${codec.name} (${codec.mimeType})`);
          break;
        }
      }

      if (!selectedCodec) {
        console.error('‚ùå No supported codec found!');
        selectedCodec = { mimeType: '', ext: 'webm', name: 'Browser default' };
      }

      try {
        const options = {
          videoBitsPerSecond: 5000000, // 5 Mbps for high quality
          audioBitsPerSecond: 128000   // 128 kbps for audio
        };

        if (selectedCodec.mimeType) {
          options.mimeType = selectedCodec.mimeType;
        }

        mediaRecorder = new MediaRecorder(combined, options);

        mediaRecorder.ondataavailable = e => {
          if (e.data.size > 0) {
            recordedChunks.push(e.data);
            console.log(`üì¶ Chunk received: ${(e.data.size / 1024 / 1024).toFixed(2)} MB`);
          }
        };

        mediaRecorder.onstop = () => {
          console.log(`üé¨ Recording stopped. Total chunks: ${recordedChunks.length}`);
          const totalSize = recordedChunks.reduce((sum, chunk) => sum + chunk.size, 0);
          console.log(`üìä Total size: ${(totalSize / 1024 / 1024).toFixed(2)} MB`);

          const blob = new Blob(recordedChunks, {
            type: selectedCodec.mimeType || 'video/webm'
          });

          recordedChunks = [];

          // Send to parent with metadata
          window.parent.postMessage({
            type: 'RECORDING_DATA',
            blob: blob,
            extension: selectedCodec.ext,
            codec: selectedCodec.name,
            size: blob.size
          }, '*');

          console.log('‚úÖ Recording sent to parent');
        };

        mediaRecorder.onerror = (e) => {
          console.error('‚ùå MediaRecorder error:', e);
        };

        mediaRecorder.onstart = () => {
          console.log('üî¥ Recording started');
        };

        mediaRecorder.onpause = () => {
          console.log('‚è∏Ô∏è  Recording paused');
        };

        mediaRecorder.onresume = () => {
          console.log('‚ñ∂Ô∏è  Recording resumed');
        };

        console.log('‚úÖ Recorder setup complete');

      } catch (e) {
        console.error('‚ùå Failed to create MediaRecorder:', e);
      }
    }

    // Texture loader removed to keep background transparent
    // CSS #background handles the image now

    document.addEventListener('DOMContentLoaded', async () => {
      await head.showAvatar({
        url: 'https://models.readyplayer.me/693fa82fe37c2412efc80488.glb?morphTargets=ARKit,Oculus+Visemes,mouthOpen,mouthSmile,eyesClosed,eyesLookUp,eyesLookDown&textureSizeLimit=1024&textureFormat=png',
        body: 'M',
        avatarMood: 'neutral',
        lipsyncLang: 'en'
      });
      loadLectureData();
    });

  </script>

  // Relay activity
  const report = () => window.parent.postMessage({ type: 'USER_ACTIVITY' }, '*');
  document.addEventListener('mousemove', report);
  document.addEventListener('touchstart', report);
  document.addEventListener('click', report);

  // Auto-resume helper for initial interaction
  document.addEventListener('click', () => {
  if (head.audioCtx && head.audioCtx.state === 'suspended' && !isPaused) {
  head.audioCtx.resume();
  console.log("AudioContext resumed by user interaction");
  }
  }, { once: true });

  </script>
</head>

<body>
  <div id="background"></div>
  <div id="avatar"></div>

  <!-- Whiteboard Board Area -->
  <div id="whiteboardBoard">
    <img src="/backgrounds/board.png" alt="Whiteboard" />
    <div id="whiteboardTextContent"></div>
  </div>

  <div id="controls">
    <button id="unlockAudio">Start</button>
    <textarea id="json" readonly></textarea>
  </div>
  <div id="loading"></div>

  <!-- Narration Text Overlay (bottom fallback) -->
  <div id="narrationOverlay">
    <p id="narrationText"></p>
  </div>
</body>

</html>