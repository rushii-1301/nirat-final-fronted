<!DOCTYPE html>
<html>

<head>
  <title>Talking Head - MP3 example</title>
  <style>
    html,
    body {
      background-color: #202020;
      margin: 0;
      padding: 0;
      font-family: Arial, sans-serif;
      width: 100%;
      height: 100%;
      overflow: hidden;
      /* Prevent scrollbars triggering resize loops */
    }

    #background {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-image: url('/backgrounds/classroom_bg.png');
      background-size: cover;
      background-position: center;
      z-index: 0;
    }

    #avatar {
      display: block;
      position: absolute;
      top: 0;
      left: 0;
      width: 30%;
      height: 100%;
      z-index: 1;
    }

    /* Whiteboard Board Image Container */
    #whiteboardBoard {
      position: absolute;
      top: 5%;
      right: 2%;
      width: 65%;
      height: 85%;
      z-index: 2;
      opacity: 0;
      transition: opacity 0.5s ease;
      pointer-events: none;
      display: flex;
      align-items: center;
      justify-content: center;
    }

    #whiteboardBoard.visible {
      opacity: 1;
    }

    /* Board backdrop blur REMOVED as per request to avoid double-board look */
    /* #whiteboardBoard::before { ... } */

    /* Actual board image */
    #whiteboardBoard img {
      width: 100%;
      height: 100%;
      object-fit: contain;
      /* Enhanced shadow for depth */
      filter: drop-shadow(0 8px 30px rgba(0, 0, 0, 0.5));
    }

    /* Text visually inside the board */
    /* Text visually inside the board */
    #whiteboardTextContent {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      width: 85%;
      max-height: 85%;

      display: flex;
      flex-direction: column;
      justify-content: flex-start;
      align-items: flex-start;

      color: #1a1a1a;
      font-size: 16px;
      line-height: 1.6;
      text-align: left;
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      font-weight: 500;

      overflow-y: auto;
      padding: 20px;
    }

    /* Title styling */
    #whiteboardTextContent .slide-title {
      font-size: 22px;
      font-weight: 700;
      color: #0a0a0a;
      margin-bottom: 15px;
      text-align: center;
      width: 100%;
      border-bottom: 2px solid #333;
      padding-bottom: 10px;
    }

    /* Bullet points styling */
    #whiteboardTextContent .slide-bullets {
      list-style: none;
      padding: 0;
      margin: 0;
      width: 100%;
    }

    #whiteboardTextContent .slide-bullets li {
      position: relative;
      padding-left: 25px;
      margin-bottom: 12px;
      font-size: 16px;
      line-height: 1.5;
    }

    #whiteboardTextContent .slide-bullets li::before {
      content: '‚Ä¢';
      position: absolute;
      left: 0;
      font-size: 20px;
      font-weight: bold;
      color: #333;
    }

    /* Scrollbar for board text */
    #whiteboardTextContent::-webkit-scrollbar {
      width: 4px;
    }

    #whiteboardTextContent::-webkit-scrollbar-thumb {
      background: rgba(0, 0, 0, 0.3);
      border-radius: 2px;
    }

    #controls {
      display: flex;
      flex-direction: column;
      gap: 10px;
      position: absolute;
      top: 50px;
      left: calc(60% + 50px);
      right: 50px;
      bottom: 50px;
      z-index: 10;
    }

    #json {
      width: 100%;
      height: 600px;
      background-color: rgba(255, 255, 255, 0.2);
      color: white;
      font-size: 14px;
      border: none;
      padding: 5px 10px;
      resize: none;
      overflow: hidden;
    }

    #loading {
      position: absolute;
      top: 50px;
      left: 50px;
      font-size: 20px;
      color: white;
      z-index: 10;
    }

    button {
      font-size: 20px;
      padding: 5px 10px;
      cursor: pointer;
    }

    #unlockAudio {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;

      opacity: 0;
      /* Invisible */
      border: none;
      outline: none;
      background: transparent;

      z-index: 999999;
      /* On top of everything */
      cursor: pointer;
    }

    /* Narration Text Overlay */
    #narrationOverlay {
      position: fixed;
      bottom: 120px;
      left: 50%;
      transform: translateX(-50%);
      max-width: 90%;
      width: auto;
      padding: 15px 25px;
      background: rgba(253, 253, 253, 0.9);
      backdrop-filter: blur(10px);
      border-radius: 12px;
      border: 1px solid rgba(255, 255, 255, 0.15);
      z-index: 5;
      opacity: 0;
      transition: opacity 0.3s ease;
      pointer-events: none;
    }

    #narrationOverlay.visible {
      opacity: 1;
    }

    #narrationText {
      color: white;
      font-size: 16px;
      line-height: 1.5;
      text-align: center;
      margin: 0;
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      max-height: 150px;
      overflow-y: auto;
    }

    /* Hide scrollbar but keep functionality */
    #narrationText::-webkit-scrollbar {
      width: 4px;
    }

    #narrationText::-webkit-scrollbar-thumb {
      background: rgba(255, 255, 255, 0.3);
      border-radius: 2px;
    }
  </style>

  <script type="importmap">
  {
    "imports": {
      "three": "https://cdn.jsdelivr.net/npm/three@0.180.0/build/three.module.js/+esm",
      "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.180.0/examples/jsm/",
      "talkinghead": "https://cdn.jsdelivr.net/gh/met4citizen/TalkingHead@1.5/modules/talkinghead.mjs"
    }
  }
  </script>

  <script type="module">
    // --- Audio Capture Logic (Monkey Patch) - MUST BE FIRST ---
    // let audioDest;
    // const originalConnect = AudioNode.prototype.connect;
    // AudioNode.prototype.connect = function (destination, outputIndex, inputIndex) {
    //   if (audioDest && destination === destination.context.destination) {
    //     try {
    //       originalConnect.call(this, audioDest);
    //     } catch (e) { }
    //   }
    //   return originalConnect.apply(this, arguments);
    // };

    import { TalkingHead } from "talkinghead";
    import * as THREE from "three";

    const nodeAvatar = document.getElementById('avatar');
    const nodeLoading = document.getElementById('loading');
    const narrationOverlay = document.getElementById('narrationOverlay');
    const narrationText = document.getElementById('narrationText');
    const whiteboardText = document.getElementById('whiteboardBoard'); // Updated to use Board Container
    const whiteboardTextContent = document.getElementById('whiteboardTextContent');

    // --- State Management ---
    const STATE = {
      IDLE: 'IDLE',
      PRELOADING: 'PRELOADING',
      PLAYING: 'PLAYING',
      PAUSED: 'PAUSED',
      WAITING_FOR_QUESTION: 'WAITING_FOR_QUESTION',
      CHAT_MODE: 'CHAT_MODE',
      CHAT_SPEAKING: 'CHAT_SPEAKING'
    };
    let currentState = STATE.IDLE;

    let lectureData = [];
    let preloadedSlides = [];
    let currentSlideIndex = 0;

    let mediaRecorder;
    let recordedChunks = [];

    // Initialize TalkingHead with dummy TTS endpoint (we'll use browser SpeechSynthesis instead)
    const head = new TalkingHead(nodeAvatar, {
      ttsEndpoint: "https://texttospeech.googleapis.com/v1beta1/text:synthesize",
      ttsApikey: "dummy", // Won't be used, but library requires it
      lipsyncModules: ["en", "fi"],
      cameraView: "full"
    });

    // audioDest = head.audioCtx.createMediaStreamDestination();

    // --- Voice Recognition Setup ---
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    let recognition;
    if (SpeechRecognition) {
      recognition = new SpeechRecognition();
      recognition.lang = 'en-IN';
      recognition.continuous = false;
      recognition.interimResults = false;

      recognition.onresult = (event) => {
        const transcript = event.results[0][0].transcript.toLowerCase();
        console.log("Voice Input:", transcript);
        const yesKeywords = ["yes", "ha", "haan", "haa", "yeah", "yep", "sure"];
        if (yesKeywords.some(kw => transcript.includes(kw))) {
          handleYesResponse();
        }
      };

      recognition.onend = () => {
        if (currentState === STATE.WAITING_FOR_QUESTION) {
          // If we finished listening and no "yes" was caught, the 5s timer will handle the fallback
        }
      };
    }

    // --- Preloading Logic ---

    async function loadLectureData() {
      if (currentState !== STATE.IDLE) return;
      currentState = STATE.PRELOADING;

      try {
        nodeLoading.textContent = "Analyzing Lecture...";
        const lectureId = new URLSearchParams(window.location.search).get('lectureId');
        if (!lectureId) throw new Error("No Lecture ID provided");

        const res = await fetch(`https://api.edinai.inaiverse.com/lectures/${lectureId}/play`);
        if (!res.ok) throw new Error("Failed to fetch lecture");
        const data = await res.json();
        const detailUrl = "https://api.edinai.inaiverse.com" + data.lecture_url;

        const detailRes = await fetch(detailUrl);
        const detailData = await detailRes.json();

        lectureData = (detailData.slides || []).map(slide => ({
          audio_url: "https://api.edinai.inaiverse.com" + slide.audio_url,
          narration: slide.narration || "",
          title: slide.title || "",
          bullets: slide.bullets || []
        }));

        await preloadSlides();

        // Only set to IDLE if we haven't started playing yet
        if (currentState === STATE.PRELOADING) {
          currentState = STATE.IDLE;
          nodeLoading.textContent = "Ready to Begin";
          // Send READY again just in case
          window.parent.postMessage({ type: 'EVT_READY' }, '*');
        }

      } catch (e) {
        console.error(e);
        nodeLoading.textContent = "Preload Error: " + e.message;
      }
    }

    async function preloadSlides() {
      for (let i = 0; i < lectureData.length; i++) {
        // If first slide is already done, show generic loading
        if (i > 0) {
          nodeLoading.textContent = `Preloading Slide ${i + 1}/${lectureData.length}...`;
        } else {
          nodeLoading.textContent = `Loading First Slide...`;
        }

        try {
          const slide = lectureData[i];
          const prepared = await prepareSlideAudio(slide.audio_url, slide.narration);
          if (prepared) preloadedSlides.push(prepared);

          // Early Ready Signal: After 1st slide
          if (i === 0) {
            console.log('‚úÖ First slide loaded - Enabling Start');
            window.parent.postMessage({ type: 'EVT_READY' }, '*');
            nodeLoading.textContent = "Ready to Begin (Background loading...)";

            // Allow state transition to IDLE or Playable so buttons work
            // But we keep 'currentState' as PRELOADING locally until loadLectureData finishes?
            // No, React side needs EVT_READY. React side doesn't care about iframe internal state for the button enable
            // But the 'CMD_START' handler in iframe needs to work.

            // We need to briefly set state to IDLE so CMD_START works?
            // Actually CMD_START logic:
            // if (mediaRecorder.state === 'inactive') { ... startLectureLoop() }
            // It doesn't strictly check 'currentState' to allow start, it checks mediaRecorder.

            // But 'loadLectureData' sets 'currentState = STATE.PRELOADING' at start.
            // Let's allow it.
          }

        } catch (e) {
          console.error("Failed to preload slide " + i, e);
        }
      }
    }

    async function prepareSlideAudio(url, narration, forcePreload = false) {
      console.log(`üì• Preparing audio: ${url}`);
      try {
        const resp = await fetch(url);
        if (!resp.ok) throw new Error(`Fetch failed: ${resp.status}`);

        const blob = await resp.blob();
        // Force mp3 mime type if missing, critical for Groq
        const mimeType = blob.type && blob.type !== "" ? blob.type : "audio/mpeg";
        const file = new File([blob], "audio.mp3", { type: mimeType });

        const ab = await file.arrayBuffer();
        const audioBuffer = await head.audioCtx.decodeAudioData(ab);

        // Hardcoded key as per previous fix
        const VITE_GROQ_API_KEY = "";

        let words = [], wtimes = [], wdurations = [];
        let gestures = [];

        try {
          const form = new FormData();
          form.append("file", file);
          form.append("model", "whisper-large-v3-turbo");
          // form.append("language", "en"); // Allow auto-detect for Hindi/Mixed
          form.append("response_format", "verbose_json");

          console.log("üìù Sending to Groq for transcription...");
          const groqResp = await fetch("https://api.groq.com/openai/v1/audio/transcriptions", {
            method: "POST",
            body: form,
            headers: { "Authorization": `Bearer ${VITE_GROQ_API_KEY}` }
          });

          if (groqResp.ok) {
            const json = await groqResp.json();
            if (json && json.segments) {
              json.segments.forEach(s => {
                words.push(s.text.trim());
                wtimes.push(Math.round(s.start * 1000));
                wdurations.push(Math.round((s.end - s.start) * 1000));
              });
              console.log(`‚úÖ Transcription success: ${words.length} words`);

              // Generate automatic gestures based on word timing
              // Use standard generic gestures "A", "B", "C" for variety
              const availableGestures = ["A", "B", "C"];
              const totalDuration = audioBuffer.duration * 1000;
              let nextGestureTime = 500;
              while (nextGestureTime < totalDuration - 1000) {
                const randomGesture = availableGestures[Math.floor(Math.random() * availableGestures.length)];
                gestures.push([nextGestureTime, 2000, randomGesture]);
                nextGestureTime += 3000 + Math.random() * 3000;
              }

            }
          } else {
            const errText = await groqResp.text();
            console.error("‚ùå Groq API Failed:", groqResp.status, errText);
          }
        } catch (e) {
          console.error("‚ùå Transcription error:", e);
        }

        // Return object compatible with head.speakAudio
        return {
          audio: audioBuffer,
          words,
          wtimes,
          wdurations,
          text: narration,
          gestures, // Include generated gestures
          preloadOnly: forcePreload // Flag to prevent auto-play during preload
        };
      } catch (e) {
        console.error("‚ùå prepareSlideAudio failed:", e);
        return null;
      }
    }

    // --- Core State Logic ---

    async function setMachineState(newState) {
      console.log(`üîÑ State Transition: ${currentState} ‚Üí ${newState}`);
      const oldState = currentState;
      currentState = newState;

      // Notify parent React component
      window.parent.postMessage({
        type: 'EVT_SYNC_STATE',
        state: newState,
        oldState: oldState,
        timestamp: Date.now(),
        slideIndex: currentSlideIndex
      }, '*');

      // Handle state-specific actions
      // ‚úÖ CRITICAL FIX: Don't suspend AudioContext during CHAT_SPEAKING
      if (newState === STATE.PAUSED || newState === STATE.WAITING_FOR_QUESTION) {
        // Pause slide playback
        console.log(`‚è∏Ô∏è  Pausing slide: Recording=${mediaRecorder?.state}, Audio=${head.audioCtx.state}`);

        if (mediaRecorder && mediaRecorder.state === 'recording') {
          try {
            mediaRecorder.pause();
            console.log('‚úÖ Recording paused');
          } catch (e) {
            console.error('‚ùå Failed to pause recording:', e);
          }
        }

        const ensureAudioContext = async () => {
          if (head.audioCtx.state !== 'running') {
            await head.audioCtx.resume();
            console.log('‚úÖ AudioContext resumed');
          }
        };

        head.stop();
        console.log('‚úÖ Avatar stopped');
      }
      else if (newState === STATE.CHAT_MODE) {
        // ‚úÖ CRITICAL: Don't suspend AudioContext - chatbot needs it!
        console.log(`üí¨ Entering chat mode: Recording=${mediaRecorder?.state}`);

        // Only pause recording, keep AudioContext ready
        if (mediaRecorder && mediaRecorder.state === 'recording') {
          try {
            mediaRecorder.pause();
            console.log('‚úÖ Recording paused for chat');
          } catch (e) {
            console.error('‚ùå Failed to pause recording:', e);
          }
        }

        // Stop slide audio but keep context running
        head.stop();
        console.log('‚úÖ Slide audio stopped, AudioContext kept running for chatbot');
      }
      else if (newState === STATE.CHAT_SPEAKING) {
        // ‚úÖ Chatbot is speaking - ensure AudioContext is running
        console.log(`ü§ñ Chatbot speaking: Audio=${head.audioCtx.state}`);

        if (head.audioCtx.state !== 'running') {
          try {
            await head.audioCtx.resume();
            console.log('‚úÖ Audio context resumed for chatbot');
          } catch (e) {
            console.error('‚ùå Failed to resume audio:', e);
          }
        }
      }
      else if (newState === STATE.PLAYING) {
        // Resume everything
        console.log(`‚ñ∂Ô∏è  Resuming: Recording=${mediaRecorder?.state}, Audio=${head.audioCtx.state}`);

        if (head.audioCtx.state === 'suspended') {
          try {
            await head.audioCtx.resume();
            console.log('‚úÖ Audio context resumed');
          } catch (e) {
            console.error('‚ùå Failed to resume audio:', e);
          }
        }

        if (mediaRecorder && mediaRecorder.state === 'paused') {
          try {
            mediaRecorder.resume();
            console.log('‚úÖ Recording resumed');
          } catch (e) {
            console.error('‚ùå Failed to resume recording:', e);
          }
        }

        head.start();
        console.log('‚úÖ Avatar started');
      }
    }

    async function startLectureLoop() {
      console.log('üé¨ Starting lecture loop...');
      console.log(`üìä Preloaded slides: ${preloadedSlides.length}`);
      console.log(`üìä Current slide index: ${currentSlideIndex}`);

      if (preloadedSlides.length === 0) {
        console.error('‚ùå No slides preloaded!');
        return;
      }

      if (head.audioCtx.state === 'suspended') {
        console.log('üîä Resuming audio context...');
        await head.audioCtx.resume();
      }

      await setMachineState(STATE.PLAYING);

      for (; currentSlideIndex < preloadedSlides.length; currentSlideIndex++) {
        const slideAudio = preloadedSlides[currentSlideIndex];
        console.log(`\nüìç Processing slide ${currentSlideIndex + 1}/${preloadedSlides.length}`);

        // Check for external Pause OR Chat Mode
        while (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) {
          await new Promise(r => setTimeout(r, 200));
        }

        if (slideAudio) {
          // Show title and bullets ON WHITEBOARD
          const currentSlide = lectureData[currentSlideIndex];
          if (currentSlide && (currentSlide.title || currentSlide.bullets?.length > 0)) {
            // Clear previous content
            whiteboardTextContent.innerHTML = '';

            // Add title if exists
            if (currentSlide.title) {
              const titleEl = document.createElement('div');
              titleEl.className = 'slide-title';
              titleEl.textContent = currentSlide.title;
              whiteboardTextContent.appendChild(titleEl);
            }

            // Add bullets if exist
            if (currentSlide.bullets && currentSlide.bullets.length > 0) {
              const bulletList = document.createElement('ul');
              bulletList.className = 'slide-bullets';
              currentSlide.bullets.forEach(bullet => {
                const li = document.createElement('li');
                li.textContent = bullet;
                bulletList.appendChild(li);
              });
              whiteboardTextContent.appendChild(bulletList);
            }

            whiteboardText.classList.add('visible');
            console.log(`üìù Displaying on whiteboard - Title: ${currentSlide.title}, Bullets: ${currentSlide.bullets?.length || 0}`);
          }

          console.log('üéµ Starting audio playback...');
          head.speakAudio(slideAudio);
          await waitForAudioEndOrPause();
          console.log('‚úÖ Audio playback finished');

          // Hide whiteboard text when slide ends
          whiteboardText.classList.remove('visible');

          // If we paused during the slide, wait here
          if (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) {
            console.log('‚è∏Ô∏è  Paused during slide, will replay');
            currentSlideIndex--; // Replay / Resume this slide
            continue;
          }

          // Slide Ended -> Trigger Prompt
          await handleSlideBoundary();

          // CRITICAL: After handleSlideBoundary, check if we entered CHAT_MODE
          // If yes, we need to HOLD here and wait for explicit resume
          while (currentState === STATE.CHAT_MODE) {
            await new Promise(r => setTimeout(r, 200));
          }

          // If we're still paused after chat, replay this slide
          if (currentState === STATE.PAUSED) {
            currentSlideIndex--;
            continue;
          }
        }
      }

      console.log('üèÅ Lecture loop completed');
      finishLecture();
    }

    async function waitForAudioEndOrPause() {
      return new Promise(resolve => {
        const check = () => {
          if (currentState === STATE.PAUSED || currentState === STATE.CHAT_MODE) { resolve(); return; }
          if (!head.isAudioPlaying) resolve();
          else setTimeout(check, 100);
        };
        check();
      });
    }

    async function handleSlideBoundary() {
      // Automatically pause all
      await setMachineState(STATE.WAITING_FOR_QUESTION);

      // ‚úÖ REMOVED: 5-second wait before question
      // Question should be asked immediately after slide ends

      // --- 1. System asks verbally using browser TTS ---
      const questionText = "Do you have any questions?";
      const utterance = new SpeechSynthesisUtterance(questionText);
      utterance.rate = 1;
      utterance.pitch = 1;
      utterance.volume = 1;
      utterance.lang = "en-US";

      window.speechSynthesis.speak(utterance);
      console.log(`üó£Ô∏è Speaking: "${questionText}"`);

      // Wait for speech to finish
      await new Promise(resolve => {
        utterance.onend = () => {
          console.log('‚úÖ Question prompt finished');
          resolve();
        };
        utterance.onerror = (e) => {
          console.error('‚ùå Speech error:', e);
          resolve();
        };

        // Fallback timeout
        setTimeout(resolve, 5000);
      });

      // Check again after speaking
      if (currentState !== STATE.WAITING_FOR_QUESTION) {
        console.log("State changed during question prompt. Aborting.");
        return;
      }

      // --- 2. Open microphone ---
      if (recognition && currentState === STATE.WAITING_FOR_QUESTION) {
        try { recognition.start(); } catch (e) { console.warn("Recognition start failed:", e); }
      }

      // --- 3. Wait for user response (10 SECONDS) ---
      console.log("‚è≥ Waiting for user response (10 seconds)...");
      let responseWait = 0;
      const maxResponseWait = 10000;  // ‚úÖ INCREASED to 10 seconds
      while (responseWait < maxResponseWait && currentState === STATE.WAITING_FOR_QUESTION) {
        await new Promise(r => setTimeout(r, 100));
        responseWait += 100;
      }

      console.log(`‚è±Ô∏è Response wait completed: ${responseWait}ms / ${maxResponseWait}ms`);

      // Cleanup recognition
      if (recognition) {
        try { recognition.stop(); } catch (e) { }
      }

      // --- 4. Auto-move to next slide ONLY if still in WAITING_FOR_QUESTION state ---
      if (currentState === STATE.WAITING_FOR_QUESTION) {
        console.log("‚úÖ No response detected after 10 seconds. Moving to next slide.");
        await setMachineState(STATE.PLAYING);
      } else {
        console.log("‚úÖ User responded or state changed. Not auto-progressing.");
      }
    }

    async function handleYesResponse() {
      if (currentState !== STATE.WAITING_FOR_QUESTION) return;
      await setMachineState(STATE.CHAT_MODE);
      window.parent.postMessage({ type: 'EVT_VOICE_TRIGGER', response: 'YES' }, '*');
    }

    function finishLecture() {
      if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
      window.parent.postMessage({ type: "EVT_LECTURE_COMPLETED" }, "*");
    }

    // --- Messaging ---

    window.addEventListener('message', async e => {
      const { type, text } = e.data;

      switch (type) {
        case 'CMD_START':
          if (!mediaRecorder) {
            setupRecorder();
            if (!mediaRecorder) {
              console.error('‚ùå Failed to setup recorder');
              return;
            }
          }

          if (mediaRecorder.state === 'inactive') {
            try {
              // Start with timeslice to ensure regular chunks
              mediaRecorder.start(1000); // Request data every 1 second
              console.log('üî¥ Recording started with 1s timeslice');
              startLectureLoop();
            } catch (e) {
              console.error('‚ùå Failed to start recording:', e);
            }
          }
          break;
        case 'CMD_PAUSE':
          await setMachineState(STATE.PAUSED);
          break;
        case 'CMD_ENTER_CHAT':
          // User manually opened chat - enter CHAT_MODE
          console.log('üí¨ Entering CHAT_MODE (manual)');
          console.log('üìä Current state before chat:', currentState);

          // ‚úÖ CRITICAL: Stop all audio when entering chat
          console.log('üõë Stopping all audio for chat mode...');
          head.stop();
          window.speechSynthesis.cancel();

          // Wait for cleanup
          await new Promise(r => setTimeout(r, 100));

          // ‚úÖ CRITICAL FIX: Don't suspend AudioContext - chatbot needs it!
          // Keep it running so chatbot audio can play
          console.log('‚úÖ AudioContext kept running for chatbot');

          await setMachineState(STATE.CHAT_MODE);
          console.log('‚úÖ Entered CHAT_MODE');
          break;
        case 'CMD_RESUME':
          // Stop any active voice recognition
          if (recognition) {
            try { recognition.stop(); } catch (e) { }
          }
          // Exit chat mode and resume playing
          if (currentState === STATE.CHAT_MODE || currentState === STATE.WAITING_FOR_QUESTION || currentState === STATE.PAUSED) {
            console.log("Resuming from", currentState);
            await setMachineState(STATE.PLAYING);
          }
          break;
        case 'CMD_CHAT_REPLY':
          const { audio_url } = e.data;

          console.log("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
          console.log("ü§ñ CMD_CHAT_REPLY RECEIVED");
          console.log("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");
          console.log("üìä Raw event data:", e.data);
          console.log("üìä Parsed data:", {
            hasAudioUrl: !!audio_url,
            audioUrl: audio_url,
            hasText: !!text,
            text: text,
            textLength: text?.length
          });
          console.log("üìä Current System State:", {
            currentState: currentState,
            audioCtxState: head.audioCtx.state,
            isAudioPlaying: head.isAudioPlaying,
            isSpeaking: head.isSpeaking
          });
          console.log("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê");


          if (audio_url) {
            try {
              // ========================================
              // STEP 1: FORCE STOP ALL AUDIO
              // ========================================
              console.log("üõë Stopping all audio sources...");
              head.stop();
              window.speechSynthesis.cancel();

              // Wait for cleanup
              await new Promise(r => setTimeout(r, 150));
              console.log("‚úÖ Audio cleanup complete");

              // ========================================
              // STEP 2: PREPARE CHATBOT AUDIO
              // ========================================
              console.log("üì• Loading chat audio from:", audio_url);
              const preparedChatAudio = await prepareSlideAudio(audio_url, text || "");

              if (!preparedChatAudio) {
                throw new Error("prepareSlideAudio returned null/undefined");
              }

              if (!preparedChatAudio.audio) {
                throw new Error("preparedChatAudio.audio is null/undefined");
              }

              console.log("‚úÖ Audio prepared successfully");

              // ========================================
              // STEP 3: CRITICAL - ENSURE AUDIOCONTEXT IS RUNNING
              // This is the key fix for the pause ‚Üí chat bug
              // ========================================
              console.log("üîä AudioContext state before:", head.audioCtx.state);

              if (head.audioCtx.state !== 'running') {
                console.log("‚ö†Ô∏è AudioContext not running, attempting resume...");

                try {
                  // Request resume
                  await head.audioCtx.resume();

                  // ‚úÖ CRITICAL: Wait for ACTUAL state change
                  // This prevents the race condition where speakAudio() is called
                  // before the context is fully running
                  let attempts = 0;
                  const maxAttempts = 30; // Increased timeout

                  while (head.audioCtx.state !== 'running' && attempts < maxAttempts) {
                    console.log(`‚è≥ Waiting for AudioContext... (${attempts + 1}/${maxAttempts}, state: ${head.audioCtx.state})`);
                    await new Promise(r => setTimeout(r, 100)); // Increased wait time
                    attempts++;
                  }

                  if (head.audioCtx.state !== 'running') {
                    throw new Error(`AudioContext failed to resume after ${maxAttempts} attempts (final state: ${head.audioCtx.state})`);
                  }

                  console.log("‚úÖ AudioContext successfully resumed");
                } catch (resumeError) {
                  console.error("‚ùå AudioContext resume error:", resumeError);
                  throw new Error(`AudioContext resume failed: ${resumeError.message}`);
                }
              } else {
                console.log("‚úÖ AudioContext already running");
              }

              // ========================================
              // STEP 4: VERIFY AUDIOCONTEXT IS RUNNING
              // ========================================
              if (head.audioCtx.state !== 'running') {
                throw new Error(`AudioContext verification failed (state: ${head.audioCtx.state})`);
              }

              console.log("üîä AudioContext state after:", head.audioCtx.state);

              // ========================================
              // STEP 5: TRANSITION TO CHAT_SPEAKING STATE
              // ========================================
              await setMachineState(STATE.CHAT_SPEAKING);
              console.log("‚úÖ State transitioned to CHAT_SPEAKING");

              // ========================================
              // STEP 6: START GLB ANIMATION
              // ========================================
              console.log("üé≠ Starting GLB animation...");
              head.start();
              await new Promise(r => setTimeout(r, 100)); // Give animation time to start
              console.log("‚úÖ GLB animation started");

              // ========================================
              // STEP 7: PLAY CHATBOT AUDIO WITH LIP-SYNC
              // ========================================
              console.log("üéµ Playing chatbot audio with lip-sync...");
              console.log("üìä Audio data:", {
                hasAudio: !!preparedChatAudio.audio,
                hasWords: preparedChatAudio.words?.length > 0,
                wordCount: preparedChatAudio.words?.length,
                hasGestures: preparedChatAudio.gestures?.length > 0,
                duration: preparedChatAudio.audio?.duration
              });

              head.speakAudio(preparedChatAudio);
              console.log("‚úÖ Chatbot audio playback started with lip-sync");

              // ========================================
              // STEP 8: VERIFY AUDIO IS ACTUALLY PLAYING
              // ========================================
              await new Promise(r => setTimeout(r, 500)); // Increased verification delay

              if (!head.isAudioPlaying) {
                console.error("‚ö†Ô∏è Audio not playing after 500ms, checking state...");
                console.error("üìä Debug state:", {
                  contextState: head.audioCtx.state,
                  isPlaying: head.isAudioPlaying,
                  isSpeaking: head.isSpeaking,
                  currentState: currentState,
                  audioBuffer: !!preparedChatAudio.audio,
                  audioBufferDuration: preparedChatAudio.audio?.duration
                });
                throw new Error("Chatbot audio failed to start playing (head.isAudioPlaying is false)");
              }

              console.log("‚úÖ Chatbot audio playing successfully with lip-sync");
              console.log("üìä Playback state:", {
                contextState: head.audioCtx.state,
                isPlaying: head.isAudioPlaying,
                isSpeaking: head.isSpeaking,
                currentState: currentState
              });

              // ========================================
              // STEP 9: MONITOR FOR AUDIO COMPLETION
              // ========================================
              const checkChatAudioEnd = setInterval(() => {
                if (!head.isAudioPlaying) {
                  clearInterval(checkChatAudioEnd);
                  console.log("‚úÖ Chatbot audio finished");

                  // Return to CHAT_MODE (waiting for next question)
                  setMachineState(STATE.CHAT_MODE).catch(e => {
                    console.error("Failed to return to CHAT_MODE:", e);
                  });
                }
              }, 100);

            } catch (err) {
              // ========================================
              // ERROR HANDLING + TTS FALLBACK
              // ========================================
              console.error("‚ùå Chatbot audio failed:", err);
              console.error("üìä Error state:", {
                message: err.message,
                contextState: head.audioCtx.state,
                isPlaying: head.isAudioPlaying,
                currentState: currentState
              });

              // Fallback to TTS if audio fails
              if (text) {
                console.log("üó£Ô∏è Falling back to TTS");

                try {
                  window.speechSynthesis.cancel();
                  head.stop();

                  // Ensure AudioContext is running for TTS
                  if (head.audioCtx.state !== 'running') {
                    await head.audioCtx.resume();

                    let attempts = 0;
                    while (head.audioCtx.state !== 'running' && attempts < 10) {
                      await new Promise(r => setTimeout(r, 50));
                      attempts++;
                    }
                  }

                  await setMachineState(STATE.CHAT_SPEAKING);

                  const chatUtterance = new SpeechSynthesisUtterance(text);
                  chatUtterance.rate = 1;
                  chatUtterance.pitch = 1;
                  chatUtterance.volume = 1;
                  chatUtterance.lang = "en-IN";

                  chatUtterance.onend = () => {
                    console.log("‚úÖ TTS finished");
                    setMachineState(STATE.CHAT_MODE).catch(e => {
                      console.error("Failed to return to CHAT_MODE:", e);
                    });
                  };

                  chatUtterance.onerror = (e) => {
                    console.error("‚ùå TTS error:", e);
                  };

                  window.speechSynthesis.speak(chatUtterance);
                  console.log("‚úÖ TTS fallback playing");

                } catch (ttsErr) {
                  console.error("‚ùå TTS fallback also failed:", ttsErr);
                }
              }
            }
          } else if (text) {
            // ========================================
            // NO AUDIO URL - USE TTS DIRECTLY
            // ========================================
            console.log("üó£Ô∏è No audio URL, using TTS");

            try {
              window.speechSynthesis.cancel();
              head.stop();

              // Ensure AudioContext is running
              if (head.audioCtx.state !== 'running') {
                await head.audioCtx.resume();

                let attempts = 0;
                while (head.audioCtx.state !== 'running' && attempts < 10) {
                  await new Promise(r => setTimeout(r, 50));
                  attempts++;
                }
              }

              await setMachineState(STATE.CHAT_SPEAKING);

              const chatUtterance = new SpeechSynthesisUtterance(text);
              chatUtterance.rate = 1;
              chatUtterance.pitch = 1;
              chatUtterance.volume = 1;
              chatUtterance.lang = "en-IN";

              chatUtterance.onend = () => {
                console.log("‚úÖ TTS finished");
                setMachineState(STATE.CHAT_MODE).catch(e => {
                  console.error("Failed to return to CHAT_MODE:", e);
                });
              };

              chatUtterance.onerror = (e) => {
                console.error("‚ùå TTS error:", e);
              };

              window.speechSynthesis.speak(chatUtterance);
              console.log("‚úÖ TTS playback started");

            } catch (ttsErr) {
              console.error("‚ùå TTS failed:", ttsErr);
            }
          }
          break;
        case 'CMD_STOP':
          finishLecture();
          break;
      }
    });

    // --- Setup ---

    // function setupRecorder() {
    //   console.log('üé¨ Setting up recorder...');

    //   const canvas = document.querySelector('#avatar canvas') || document.querySelector('canvas');
    //   if (!canvas) {
    //     console.error('‚ùå Canvas not found!');
    //     return;
    //   }

    //   const videoStream = canvas.captureStream(30);
    //   const combined = new MediaStream([
    //     ...videoStream.getVideoTracks(),
    //     ...audioDest.stream.getAudioTracks()
    //   ]);

    //   // Try different codecs in order of preference
    //   const codecOptions = [
    //     { mimeType: 'video/mp4;codecs="avc1.42E01E,mp4a.40.2"', ext: 'mp4', name: 'MP4 (H.264)' },
    //     { mimeType: 'video/webm;codecs=vp9,opus', ext: 'webm', name: 'WebM (VP9)' },
    //     { mimeType: 'video/webm;codecs=vp8,opus', ext: 'webm', name: 'WebM (VP8)' },
    //     { mimeType: 'video/webm', ext: 'webm', name: 'WebM (default)' }
    //   ];

    //   let selectedCodec = null;
    //   for (const codec of codecOptions) {
    //     if (MediaRecorder.isTypeSupported(codec.mimeType)) {
    //       selectedCodec = codec;
    //       console.log(`‚úÖ Using codec: ${codec.name} (${codec.mimeType})`);
    //       break;
    //     }
    //   }

    //   if (!selectedCodec) {
    //     console.error('‚ùå No supported codec found!');
    //     selectedCodec = { mimeType: '', ext: 'webm', name: 'Browser default' };
    //   }

    //   try {
    //     const options = {
    //       videoBitsPerSecond: 5000000, // 5 Mbps for high quality
    //       audioBitsPerSecond: 128000   // 128 kbps for audio
    //     };

    //     if (selectedCodec.mimeType) {
    //       options.mimeType = selectedCodec.mimeType;
    //     }

    //     mediaRecorder = new MediaRecorder(combined, options);

    //     mediaRecorder.ondataavailable = e => {
    //       if (e.data.size > 0) {
    //         recordedChunks.push(e.data);
    //         console.log(`üì¶ Chunk received: ${(e.data.size / 1024 / 1024).toFixed(2)} MB`);
    //       }
    //     };

    //     mediaRecorder.onstop = () => {
    //       console.log(`üé¨ Recording stopped. Total chunks: ${recordedChunks.length}`);
    //       const totalSize = recordedChunks.reduce((sum, chunk) => sum + chunk.size, 0);
    //       console.log(`üìä Total size: ${(totalSize / 1024 / 1024).toFixed(2)} MB`);

    //       const blob = new Blob(recordedChunks, {
    //         type: selectedCodec.mimeType || 'video/webm'
    //       });

    //       recordedChunks = [];

    //       // Send to parent with metadata
    //       window.parent.postMessage({
    //         type: 'RECORDING_DATA',
    //         blob: blob,
    //         extension: selectedCodec.ext,
    //         codec: selectedCodec.name,
    //         size: blob.size
    //       }, '*');

    //       console.log('‚úÖ Recording sent to parent');
    //     };

    //     mediaRecorder.onerror = (e) => {
    //       console.error('‚ùå MediaRecorder error:', e);
    //     };

    //     mediaRecorder.onstart = () => {
    //       console.log('üî¥ Recording started');
    //     };

    //     mediaRecorder.onpause = () => {
    //       console.log('‚è∏Ô∏è  Recording paused');
    //     };

    //     mediaRecorder.onresume = () => {
    //       console.log('‚ñ∂Ô∏è  Recording resumed');
    //     };

    //     console.log('‚úÖ Recorder setup complete');

    //   } catch (e) {
    //     console.error('‚ùå Failed to create MediaRecorder:', e);
    //   }
    // }



    function setupRecorder() {
      console.log('üé¨ Setting up recorder (Video Only Mode for Lip-Sync Safety)...');

      const canvas = document.querySelector('#avatar canvas') || document.querySelector('canvas');
      if (!canvas) {
        console.error('‚ùå Canvas not found!');
        return;
      }

      // Sirf Video Stream lein (Audio hata diya taaki Lip-Sync na tute)
      const videoStream = canvas.captureStream(30);

      // üü¢ CHANGE: Humne audioDest hata diya hai
      const combined = new MediaStream([
        ...videoStream.getVideoTracks()
        // Audio tracks removed temporarily to fix lip-sync
      ]);

      // Codecs logic same rahega
      const codecOptions = [
        { mimeType: 'video/mp4;codecs="avc1.42E01E,mp4a.40.2"', ext: 'mp4', name: 'MP4 (H.264)' },
        { mimeType: 'video/webm;codecs=vp9,opus', ext: 'webm', name: 'WebM (VP9)' },
        { mimeType: 'video/webm', ext: 'webm', name: 'WebM (default)' }
      ];

      let selectedCodec = null;
      for (const codec of codecOptions) {
        if (MediaRecorder.isTypeSupported(codec.mimeType)) {
          selectedCodec = codec;
          break;
        }
      }

      if (!selectedCodec) {
        selectedCodec = { mimeType: '', ext: 'webm', name: 'Browser default' };
      }

      try {
        mediaRecorder = new MediaRecorder(combined, {
          mimeType: selectedCodec.mimeType,
          videoBitsPerSecond: 5000000
        });

        mediaRecorder.ondataavailable = e => {
          if (e.data.size > 0) recordedChunks.push(e.data);
        };

        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: selectedCodec.mimeType });
          recordedChunks = [];
          window.parent.postMessage({
            type: 'RECORDING_DATA',
            blob: blob,
            extension: selectedCodec.ext
          }, '*');
        };

        console.log('‚úÖ Recorder setup complete (Video Only)');
      } catch (e) {
        console.error('‚ùå Failed to create MediaRecorder:', e);
      }
    }

    // Texture loader removed to keep background transparent
    // CSS #background handles the image now

    document.addEventListener('DOMContentLoaded', async () => {
      await head.showAvatar({
        url: 'https://models.readyplayer.me/692dcb2e176ba02c5bc925c0.glb?morphTargets=ARKit,Oculus+Visemes,mouthOpen,mouthSmile,eyesClosed,eyesLookUp,eyesLookDown&textureSizeLimit=1024&textureFormat=png',
        body: 'M',
        avatarMood: 'neutral',
        lipsyncLang: 'en'
      });
      loadLectureData();
    });


    // Relay activity
    const report = () => window.parent.postMessage({ type: 'USER_ACTIVITY' }, '*');
    document.addEventListener('mousemove', report);
    document.addEventListener('touchstart', report);
    document.addEventListener('click', report);

    // Auto-resume helper for initial interaction
    document.addEventListener('click', () => {
      if (head.audioCtx && head.audioCtx.state === 'suspended' && !isPaused) {
        head.audioCtx.resume();
        console.log("AudioContext resumed by user interaction");
      }
    }, { once: true });

  </script>
</head>

<body>
  <div id="background"></div>
  <div id="avatar"></div>

  <!-- Whiteboard Board Area -->
  <div id="whiteboardBoard">
    <img src="/backgrounds/board.png" alt="Whiteboard" />
    <div id="whiteboardTextContent"></div>
  </div>

  <!-- <div id="controls">
    <button id="unlockAudio">Start</button>
    <textarea id="json" readonly></textarea>
  </div> -->
  <div id="loading"></div>

  <!-- Narration Text Overlay (bottom fallback) -->
  <div id="narrationOverlay">
    <p id="narrationText"></p>
  </div>
</body>

</html>